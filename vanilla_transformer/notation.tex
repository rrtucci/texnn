\documentclass[12pt]{article}
\input{bayesuvius.sty}
\begin{document}

Our tensor notation is discussed in Section 
\ref{sec-numpy-tensors} of Bayesuvius.

$\ell=$ number of words in a sentence segment. $\alp\in [\ell]$, $\ell\sim 100$

$L=$ number of words in vocabulary, $\beta\in[L]$, $L>> \ell$

$d=d_\rvq=d_\rvk=d_\rvv=64$, hidden dimension  per head,
$\delta\in[d]$. 

$n_\rvh=8$ number of heads, $\nu \in[n_\rvh]$

$D=d=8(64)=512$, hidden dimension for all heads. 
$\Delta\in [D]$


\beq
e^{\delta, \alp} = \sum_\beta 
E^{\delta, \beta}
x^{\beta, \alp}
\eeq

\beq
Q^{\nu,\delta, \alp}=\sum_{\delta'}
W_\rvq^{\nu, \delta, \delta'}e^{\delta', \alp}
\eeq


\beq
K^{\nu,\delta, \alp}=
\sum_{\delta'}
W_\rvk^{\nu, \delta, \delta'}
e^{\delta', \alp}
\eeq

\beq
V^{\nu,\delta, \alp}=
\sum_{\delta'}
W_\rvv^{\nu, \delta, \delta'}
e^{\delta', \alp}
\eeq



\beq
B^{
\nu,\alp, \alp'}=
\frac{
\sum_\delta Q^{\nu,\delta,\alp}
K^{\nu,\delta,\alp'}
}{\sqrt{d}}
\eeq

\beqa
A^{\nu, [d], [\ell]}&=&
\sum_\alp
V^{\nu, [d], \alp}
\underbrace{
{\rm softmax}(B^{\nu, \alp ,[\ell]})}_{
(B^*)^{\nu, \alp, [\ell]}}
\\
&=&
V^{\nu, [d], [\ell]}(B^*)^{\nu, [\ell], [\ell]}
\eeqa


\beq
A^{[n_\rvh], [d], [\ell]} \rarrow A^{[D], [\ell]}
\eeq


\beq
Q^{[D], [\ell]}=
W_\rvq^{[D],[d]}E^{[d],[\ell]}
\eeq

\beq
K^{[D], [\ell]}=
W_\rvk^{[D],[d]}E^{[d],[\ell]}
\eeq

\beq
V^{[D], [\ell]}=
W_\rvv^{[D],[d]}E^{[d],[\ell]}
\eeq

Positional Encoding Matrix 
$E_{pos}^{[d],[\ell]}$

\beq
E_{pos}^{\delta, \beta}=
\left\{
\begin{array}{ll}
\sin\left(\frac{\beta}
{10^{4\delta/d}}\right)= \sin(2\pi \frac{\beta}{W\ell(\delta)})
& \text{if $\delta$ is even}
\\
\cos\left(\frac{\beta}{10^{4(\delta-1)/d}}\right)=
\cos(2\pi\frac{\beta}{W\ell(\delta)})
& \text{if $\delta$ is odd}
\end{array}
\right.
\eeq
$E_{pos}^{\delta, \beta}$ changes in phase by $\pi/2$  
every time $\delta$ changes by 1. Its wavelength 
$W\ell$ is independent
of $\ell$, but increases rapidly with $\delta$, from $W\ell(\delta=0)=2\pi*1$ to 
$W\ell(\delta=d)= 2\pi* 10^4$.

For a tensor $T$ of arbitrary shape
\beq
ReLU(T) = (T)_+ = max(0, T)
\eeq
max element-wise

Feed Forward neural net
\beq
F(x^{[1], [\ell]}) = ReLU(x^{[1],[\ell]}
W_1^{[\ell], [d]} + b_1^{[1],[d]}) W_2^{[d], [\ell]} + b_1^{[1],[\ell]}
\eeq

\beq
F(x^{[\ell]}) = W_2^{[\ell], [d]}ReLU(
W_1^{[d], [\ell]}x^{[\ell]} + b_1^{[d]})  + b_1^{[\ell]}
\eeq



\end{document}