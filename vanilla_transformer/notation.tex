\documentclass[12pt]{article}
\input{bayesuvius.sty}
\begin{document}

Our tensor notation is discussed in Section 
\ref{sec-numpy-tensors} of Bayesuvius.

$L=$ number of words in a segment, $\lam\in[L]$

$d=64$ hidden dimension  per head,
$\delta\in[d]$. $d_\rvq=d_\rvk=d_\rvv=d$

$n=8$ number of heads, $\nu \in[n]$

$D=nd=8(64)=512$, hidden dimension for all heads. 
$\Delta\in [D]$


\beq
e^\delta = \sum_\lam E^{\delta \lam}x^\lam
\eeq

\beq
Q^{\nu,\delta, \lam}=\sum_{\delta'}
W_\rvq^{\nu, \delta, \delta'}E^{\delta', \lam}
\eeq


\beq
K^{\nu,\delta, \lam}=
\sum_{\delta'}
W_\rvk^{\nu, \delta, \delta'}
E^{\delta', \lam}
\eeq

\beq
V^{\nu,\delta, \lam}=
\sum_{\delta'}
W_\rvv^{\nu, \delta, \delta'}
E^{\delta', \lam}
\eeq



\beq
B^{
\nu,\lam, \lam'}=
\frac{
\sum_\delta Q^{\nu,\delta,\lam}
K^{\nu,\delta,\lam'}
}{\sqrt{d}}
\eeq

\beqa
A^{\nu, [d], [L]}&=&
\sum_\lam
V^{\nu, [d], \lam}
\underbrace{
{\rm softmax}(B^{\nu, \lam ,[L]})}_{
(B^*)^{\nu, \lam, [L]}}
\\
&=&
V^{\nu, [d], [L]}(B^*)^{\nu, [L], [L]}
\eeqa


\beq
A^{[n], [d], [L]} \rarrow A^{[D], [L]}
\eeq


\beq
Q^{[D], [L]}=
W_\rvq^{[D],[d]}E^{[d],[L]}
\eeq

\beq
K^{[D], [L]}=
W_\rvk^{[D],[d]}E^{[d],[L]}
\eeq

\beq
V^{[D], [L]}=
W_\rvv^{[D],[d]}E^{[d],[L]}
\eeq

Positional Encoding Matrix 
$E_{pos}^{[d],[L]}$

\beq
E_{pos}^{\delta, \lam}=
\left\{
\begin{array}{ll}
\sin\left(\frac{\lam}
{10^{4\delta/d}}\right)= \sin(2\pi \frac{\lam}{WL(\delta)})
& \text{if $\delta$ is even}
\\
\cos\left(\frac{\lam}{10^{4(\delta-1)/d}}\right)=
\cos(2\pi\frac{\lam}{WL(\delta)})
& \text{if $\delta$ is odd}
\end{array}
\right.
\eeq
$E_{pos}^{\delta, \lam}$ changes in phase by $\pi/2$  
every time $\delta$ changes by 1. Its wavelength 
$WL$ is independent
of $L$, but increases rapidly with $\delta$, from $WL(\delta=0)=2\pi*1$ to 
$WL(\delta=d)= 2\pi* 10^4$.

For a tensor $T$ of arbitrary shape
\beq
ReLU(T) = (T)_+ = max(0, T)
\eeq
max element-wise

Feed Forward neural net
\beq
F(x^{[1], [L]}) = ReLU(x^{[1],[L]}
W_1^{[L], [d]} + b_1^{[1],[d]}) W_2^{[d], [L]} + b_1^{[1],[L]}
\eeq

\beq
F(x^{[L]}) = W_2^{[L], [d]}ReLU(
W_1^{[d], [L]}x^{[L]} + b_1^{[d]})  + b_1^{[L]}
\eeq



\end{document}