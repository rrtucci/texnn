\documentclass[12pt]{article}
\input{bayesuvius.sty}
\begin{document}

Our tensor notation is discussed in Section 
\ref{sec-numpy-tensors} of Bayesuvius.

TA = Transformer Architecture

$\ell=$ maximum number of words in a sentence segment. $\alp\in [\ell]$, $\ell\sim 100$

$L=$ number of words in vocabulary, $\beta\in[L]$, $L>> \ell$

$d=d_\rvq=d_\rvk=d_\rvv=64$, hidden dimension  per head,
$\delta\in[d]$. 

$n_\rvh=8$, number of heads, $\nu \in[n_\rvh]$

$D=n_\rvh d=8(64)=512$, hidden dimension for all heads,
$\Delta\in [D]$

$\Lambda=6$, number of layers in plate (a.k.a., stack), $\lam\in[\Lambda]$

reshaping
\beq
T^{\nu, \delta}\rarrow T^{\Delta}
\;\;
\left(
T^{[n_\rvh], [d]} \rarrow T^{[D]}
\right)
\eeq

\beq
T^{\Delta}\rarrow T^{\nu, \delta}
\;\;
\left(
T^{[D]}\rarrow T^{[n_\rvh], [d]}
\right)
\eeq

concatenation
\beq
T^{[n]}= (T^0, T^1,\ldots, T^{n-1})= 
(T^\nu)_{\nu\in[n]}
\eeq

Hadamard product (element-wise, entry-wise multiplication)
\beq
T^{[n]}* S^{[n]}= (T^\nu S^\nu)_{\nu\in[n]}
\eeq


Matrix multiplication
( $T^{[n]}= T^{[n], [1]}$ is a column vector)

\beq
(T^{[n]})^T S^{[n]}=\text{scalar}
\eeq

\beq
T^{[a],[b]}S^{[b], [c]}
=\left[\sum_{\beta\in[b]} T^{\alp, \beta}
S^{\beta, \gamma}\right]
_{\alp_\in [a], \gamma \in [c]}
\eeq

Most treatments of TA, including the
Transformers is All You Need paper,  order the
operations chronologically from
left to right. So if $A$ occurs before $B$,
they write $AB$.
This is contrary 
to what is done in Linear Algebra, where one 
orders the operations chronologically from right to left, and one writes $BA$.
We will adhere to the Linear Algebra
convention, since it is so prevalent
and is the overwhelming precedent.


\beq
x^{\beta, \alp} =\delta(\beta, \beta(\alp))
\left(
x^{[L], [\ell]} \text{ has one hot columns.}
\right)
\eeq

\beq
e^{\delta, \alp} = \sum_\beta 
E^{\delta, \beta}
x^{\beta, \alp}
\;\;
\left(
e^{[d],[\ell]}= E^{[d], [L]}x^{[L], [\ell]}
\right)
\eeq

\beq
Q^{\nu,\delta, \alp}=\sum_{\delta'}
W_\rvq^{\nu, \delta, \delta'}e^{\delta', \alp}
\;\;
\left(
Q^{[D], [\ell]}=
W_\rvq^{[D],[d]}E^{[d],[\ell]}
\right)
\eeq


\beq
K^{\nu,\delta, \alp}=
\sum_{\delta'}
W_\rvk^{\nu, \delta, \delta'}
e^{\delta', \alp}
\;\;
\left(
K^{[D], [\ell]}=
W_\rvk^{[D],[d]}E^{[d],[\ell]}
\right)
\eeq

\beq
V^{\nu,\delta, \alp}=
\sum_{\delta'}
W_\rvv^{\nu, \delta, \delta'}
e^{\delta', \alp}
\;\;
\left(
V^{[D], [\ell]}=
W_\rvv^{[D],[d]}E^{[d],[\ell]}
\right)
\eeq



\beq
B^{
\nu,\alp, \alp'}=
\frac{1}{\sqrt{d}}
\sum_\delta K^{\nu,\delta,\alp}
Q^{\nu,\delta,\alp'}
\;\;
\left(
B^{[n_h],[\ell],[\ell]}
=\left[
\frac{1}{\sqrt{d}}
(K^{\nu, [d], [\ell]})^T
Q^{\nu, [d], [\ell]}\right]_{\nu\in[n_\rvh]}
\right)
\eeq

\beqa
A^{\nu, 
\delta, \alp'}&=&
\sum_{\alp}
V^{\nu, \delta, \alp}
\underbrace{
{\rm softmax}(B^{\nu, \alp ,\alp'})}_{
P(\alp|\nu, \alp')}
\eeqa

\beq
\sum_{\alp\in [\ell]}P(\alp|\nu, \alp')=1
\eeq

\beq
A^{\nu, \delta, \alp'}
\rarrow
A^{\Delta, \alp'}
\left(
A^{[n_\rvh], [d], [\ell]}
\rarrow
A^{[D], [\ell]}\right)
\eeq

Column vector notation:

\beq
B^{\nu, \alp, \alp'}=
\frac{1}{\sqrt{d}}
(K^{\nu, [d], \alp})^T Q^{\nu, [d], \alp'}
\eeq
Important: Note that the softmax() makes the
$\alp$ component a probability,
not the $\alp'$ one!

For example, suppose $\nu=1$ (one head), $\ell=2$ (a 2 word segment), 
and $d=3$ (hidden dimension is 3).
The $Q^{[3], [2]}, K^{[3], [2]}, V^{[3], [2]}$ are $3\times 2$ matrices
(i.e, 2 3-dim column vectors).
One uses the $Q^{[3], [2]}$ and $K^{[3], [2]}$ to arrive at a 
$2\times 2$ matrix $P(\alp'|\alp)$
of probabilities.
Then one uses that matrix of probabilities to replace

\beq
\left[
V^{[3], 0}, V^{[3], 1} 
\right]
\rarrow
\left[
V^{[3], 0} P(0|0)
+ 
V^{[3], 1}P(1|0)
,
V^{[3], 0} P(0|1)
+ 
V^{[3], 1}P(1|1)
\right]
\eeq

\begin{itemize}
\item{\bf Positional Enbedding (a.k.a. encoding) Matrix}
 
$E_{pos}^{[d],[\ell]}$

\beq
E_{pos}^{\delta, \beta}=
\left\{
\begin{array}{ll}
\sin\left(\frac{\beta}
{10^{4\delta/d}}\right)= \sin(2\pi \frac{\beta}{\lam(\delta)})
& \text{if $\delta$ is even}
\\
\cos\left(\frac{\beta}{10^{4(\delta-1)/d}}\right)=
\cos(2\pi\frac{\beta}{\lam(\delta)})
& \text{if $\delta$ is odd}
\end{array}
\right.
\eeq
$E_{pos}^{\delta, \beta}$ changes in phase by $\pi/2$  
every time $\delta$ changes by 1. Its wavelength 
$\lam$ is independent
of $\beta$, but increases rapidly with $\delta$, from $\lam(\delta=0)=2\pi*1$ to 
$\lam(\delta=d)= 2\pi* 10^4$.

Total Enbedding equals initial enbedding plus 
positional enbedding:$E = E_0 + E_{pos}$


The purpose of positional enbedding is to take $x^{\beta, \alp}$ to $e^{\delta, \alp}=
\sum_\beta E^{\delta,\beta}_{pos} x^{\beta, \alp}$
where $e^{\delta, \alp}$ changes quickly as $\delta$ (i.e., position) changes.

\item {\bf ReLU}

For a tensor $T$ of arbitrary shape
\beq
ReLU(T) = (T)_+ = max(0, T)
\eeq
max element-wise

\item {\bf Feed Forward neural net}
\beq
F(x^{[1], [\ell]}) = ReLU(x^{[1],[\ell]}
W_1^{[\ell], [d]} + b_1^{[1],[d]}) W_2^{[d], [\ell]} + b_1^{[1],[\ell]}
\eeq

\beq
F(x^{[\ell]}) = W_2^{[\ell], [d]}ReLU(
W_1^{[d], [\ell]}x^{[\ell]} + b_1^{[d]})  + b_1^{[\ell]}
\eeq

\item {\bf Softmax}

softmax() takes a vector and returns
a vector of probabilities of the same length

\beq
x^{[n]}\rarrow P^{[n]}
\eeq
where

\beq
P^\alp=
\frac{\exp(x^\alp)}
{\sum_{\alp\in[n]} \exp(x^\alp )}
\;\;
\left(P^{[n]}=\frac{\exp(x^{[n]})}
{\norm{\exp(x^{[n]})
}_0}
\right)
\eeq

For example,
\beq
(1,0,0)\rarrow (e,1,1)/norm
\eeq

\beq
(10,0,0)\rarrow (e^{10}, 1, 1)/norm \approx (1,0,0)
\eeq

For any $a\in\RR$,
\beq
(a,a,a)\rarrow (1, 1, 1)/3
\eeq


\item {\bf Skip Connection (Add \& Normalize)}

A {\bf skip connection} is when you split the
input to a {\bf filter} into two streams, one stream goes through
the filter, the other doesn't. The one that doesn't
is then merged with the output of the filter via a {\bf add \& normalize} node. The reason for making skip connections
is that the signal exiting a filter is usually full of
jumps and kinks. By merging that filter output
with some  of the filter input, one smooths out the filter output
to some degree. This makes back-propagation differentiation
better behaved.

The filter might be a Multi-Head Attention or a Feed Forward NN.

Add \& Normalize just means $(A + B)/norm$ where $A$ and $B$
are the two input signals and ``norm" is some norm of $A+B$ (for
instance, $\norm{A+B}_2$).

Normalization keeps the signal from growing too big and saturating the signal entering components upstream.
Normalization can also involve subtracting the mean $\av{X}$ of the signal $X$  so as to get a signal $X-\av{X}$  that has zero mean.

\item {\bf Redundancy}

For better results, the Decoder contains a pair of multi-head attentions in series, and $\Lam$ of those pairs in parallel.



\end{itemize}




\end{document}