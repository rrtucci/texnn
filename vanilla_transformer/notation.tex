\documentclass[12pt]{article}
\input{bayesuvius.sty}
\begin{document}

Our tensor notation is discussed in Section 
\ref{sec-numpy-tensors} of Bayesuvius.



$\ell=$ number of words in a sentence segment. $\alp\in [\ell]$, $\ell\sim 100$

$L=$ number of words in vocabulary, $\beta\in[L]$, $L>> \ell$

$d=d_\rvq=d_\rvk=d_\rvv=64$, hidden dimension  per head,
$\delta\in[d]$. 

$n_\rvh=8$, number of heads, $\nu \in[n_\rvh]$

$D=n_\rvh d=8(64)=512$, hidden dimension for all heads. 
$\Delta\in [D]$

$\Lambda=6$, number of layers in plate (a.k.a., stack)

reshaping
\beq
T^{\nu, \delta}\rarrow T^{\Delta(\nu, \delta)}
\;\;
\left(
T^{[n_\rvh], [d]} \rarrow T^{[D]}
\right)
\eeq

\beq
T^{\Delta(\nu, \delta)}\rarrow T^{\nu, \delta}
\;\;
\left(
T^{[D]}\rarrow T^{[n_\rvh], [d]}
\right)
\eeq

concatenation
\beq
T^{[n]}= (T^0, T^1,\ldots, T^{n-1})= 
(T^\nu)_{\nu\in[n]}
\eeq

Hadamard product (element-wise, entry-wise multiplication)
\beq
T^{[n]}* S^{[n]}= (T^\nu S^\nu)_{\nu\in[n]}
\eeq


Matrix multiplication
( $T^{[n]}= T^{[n], [1]}$ is a column vector)

\beq
(T^{[n]})^T S^{[n]}=\text{scalar}
\eeq

\beq
T^{[a],[b]}S^{[b], [c]}
=\left[\sum_{\beta\in[b]} T^{\alp, \beta}
S^{\beta, \gamma}\right]
_{\alp_\in [a], \gamma \in [c]}
\eeq


\beq
e^{\delta, \alp} = \sum_\beta 
E^{\delta, \beta}
x^{\beta, \alp}
\;\;
\left(
e^{[d],[\ell]}= E^{[d], [L]}x^{[L], [\ell]}
\right)
\eeq

\beq
Q^{\nu,\delta, \alp}=\sum_{\delta'}
W_\rvq^{\nu, \delta, \delta'}e^{\delta', \alp}
\;\;
\left(
Q^{[D], [\ell]}=
W_\rvq^{[D],[d]}E^{[d],[\ell]}
\right)
\eeq


\beq
K^{\nu,\delta, \alp}=
\sum_{\delta'}
W_\rvk^{\nu, \delta, \delta'}
e^{\delta', \alp}
\;\;
\left(
K^{[D], [\ell]}=
W_\rvk^{[D],[d]}E^{[d],[\ell]}
\right)
\eeq

\beq
V^{\nu,\delta, \alp}=
\sum_{\delta'}
W_\rvv^{\nu, \delta, \delta'}
e^{\delta', \alp}
\;\;
\left(
V^{[D], [\ell]}=
W_\rvv^{[D],[d]}E^{[d],[\ell]}
\right)
\eeq



\beq
B^{
\nu,\alp, \alp'}=
\frac{1}{\sqrt{d}}
\sum_\delta Q^{\nu,\delta,\alp}
K^{\nu,\delta,\alp'}
\;\;
\left(
B^{[n_h],[\ell],[\ell]}
=\left[
\frac{1}{\sqrt{d}}
(Q^{\nu, [d], [\ell]})^T
K^{\nu, [d], [\ell]}\right]_{\nu\in[n_\rvh]}
\right)
\eeq

\beqa
A^{[n_\rvh], [d], [\ell]}&=&
\left[
\sum_\alp
V^{\nu, [d], \alp}
\underbrace{
{\rm softmax}(B^{\nu, \alp ,[\ell]})}_{
(B^*)^{\nu, \alp, [\ell]}}
\right]_{\nu\in[n_\rvh]}
\\
&=&
\left[
V^{\nu, [d], [\ell]}(B^*)^{\nu, [\ell], [\ell]}
\right]_{\nu\in[n_\rvh]}
\eeqa

\beq
A^{[n_\rvh], [d], [\ell]}
\rarrow
A^{[D], [\ell]}
\eeq

\begin{itemize}
\item{\bf Positional Encoding Matrix}
 
$E_{pos}^{[d],[\ell]}$

\beq
E_{pos}^{\delta, \beta}=
\left\{
\begin{array}{ll}
\sin\left(\frac{\beta}
{10^{4\delta/d}}\right)= \sin(2\pi \frac{\beta}{\lam(\delta)})
& \text{if $\delta$ is even}
\\
\cos\left(\frac{\beta}{10^{4(\delta-1)/d}}\right)=
\cos(2\pi\frac{\beta}{\lam(\delta)})
& \text{if $\delta$ is odd}
\end{array}
\right.
\eeq
$E_{pos}^{\delta, \beta}$ changes in phase by $\pi/2$  
every time $\delta$ changes by 1. Its wavelength 
$\lam$ is independent
of $\beta$, but increases rapidly with $\delta$, from $\lam(\delta=0)=2\pi*1$ to 
$\lam(\delta=d)= 2\pi* 10^4$.

\item {\bf ReLU}

For a tensor $T$ of arbitrary shape
\beq
ReLU(T) = (T)_+ = max(0, T)
\eeq
max element-wise

\item {\bf Feed Forward neural net}
\beq
F(x^{[1], [\ell]}) = ReLU(x^{[1],[\ell]}
W_1^{[\ell], [d]} + b_1^{[1],[d]}) W_2^{[d], [\ell]} + b_1^{[1],[\ell]}
\eeq

\beq
F(x^{[\ell]}) = W_2^{[\ell], [d]}ReLU(
W_1^{[d], [\ell]}x^{[\ell]} + b_1^{[d]})  + b_1^{[\ell]}
\eeq

\item {\bf softmax}

softmax() takes a vector and returns
a vector of probabilities of the same length

\beq
x^{[n]}\rarrow P^{[n]}
\eeq
where

\beq
P^\alp=
\frac{\exp(x^\alp)}
{\sum_{\alp\in[n]} \exp(x^\alp )}
\;\;
\left(P^{[n]}=\frac{\exp(x^{[n]})}
{\norm{\exp(x^{[n]})
}_0}
\right)
\eeq

For example,
\beq
(1,0,0)\rarrow (e,1,1)/norm
\eeq

\beq
(10,0,0)\rarrow (e^{10}, 1, 1)/norm \approx (1,0,0)
\eeq

For any $a\in\RR$,
\beq
(a,a,a)\rarrow (1, 1, 1)/3
\eeq
\end{itemize}




\end{document}