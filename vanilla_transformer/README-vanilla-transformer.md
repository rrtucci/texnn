This "vanilla transformer" 
example of texnn usage was written for a chapter of my
free, open source book [Bayesuvius](https://github.com/rrtucci/Bayesuvius).

This example is based on the following Refs. (all excellent, in their own way):
1. [Abhishek Gupta blog post "Explanation of “Attention Is All You Need” 
   with Code by Abhishek Thakur"](https://sargupta93.medium.com/explanation-of-attention-is-all-you-need-with-code-by-abhishek-thakur-89861d24ea9d)

2. [Lilian Weng blog post, "The transformer family V.2"](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/)

3. [Famous "Attention is all you need" paper](https://arxiv.org/abs/1706.03762)

4. [Transformers from scratch](https://e2eml.school/transformers.html)

5. [graphdeplearning](https://graphdeeplearning.github.io/post/transformers-are-gnns/)

6. [Wikipedia article on transformers](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))

It uses this figure from the "Attention is all you need" paper
![vanilla transformer](transformer.png)

