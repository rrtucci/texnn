\documentclass[12pt]{article}
\input{bayesuvius.sty}
\begin{document}

\begin{figure}[h!]\centering
$$\xymatrix@R=2.5pc@C=3.5pc{
&&
\\
&&
\\
&*+[F*:Dandelion]{\underline{M}^{[86], [300]}}\ar[uu]\ar[r]^{W_{il}}&*+[F*:SkyBlue]{\underline{L}^{[86], [6]}}\ar[uu]
\\
*+[F*:pink]{\underline{S}^{[86], [768]}}\ar[ur]^{W_{me}}&*+[F*:pink]{\underline{E}^{[86], [768]}}\ar[l]^{1}&*+[F*:yellow]{\underline{a}^{[86]}}\ar[l]
\\
*+[F*:Orchid]{\underline{d}^{[121], [768]}}\ar[r]&*+[F*:pink]{\underline{G}^{[86], [768]}}\ar@[red]@/_5pc/[uu]|-{\color{red} W_{me}}\ar[ul]^{1}&
\\
&*+[F*:Orchid]{\underline{I}^{[121], [768]}}\ar[ul]&
\\
&*+[F*:Orchid]{\underline{B}^{[121], [768]}}\ar[u]&*+[F*:SkyBlue]{\underline{X}^{[86], [6]}}\ar[uuu]
\save
\POS"3,1"."6,1"."3,3"."6,3"!C*+<4.8em>\frm{-,}
\POS"6,2"."6,2"."6,2"."6,2"!C*+<3.8em>\frm{--}
\POS"4,1"."4,1"."4,3"."4,3"!C*+<1.0em>\frm{.}
\restore
}$$
\caption{Sax bnet. 2 copies of dashed box are connected in series. 5 copies (5 depths) of plain box are connected in series.  However, in the first of those 5 plain box copies, the dotted box  is omitted and node $\ul{G}$ feeds directly into node  $\ul{M}$ (indicated by red arrow). We display the tensor shape superscripts in the PyTorch L2R order. All tensor shape superscripts have been simplified by omitting a $[s_{ba}]$ from their left side, where $s_{ba}=24$ is the batch size. }
\label{fig-texnn-for-sentence-ax-bnet}
\end{figure}

\begin{tabular}{ll}
$\underline{a}^{[86]}$ :&{\tt ll\_greedy\_ilabel}\\
$\underline{B}^{[121], [768]}$ :&{\tt lll\_hidstate}\\
$\underline{d}^{[121], [768]}$ :&{\tt lll\_hidstate}\\
$\underline{E}^{[86], [768]}$ :&{\tt lll\_pred\_code}\\
$\underline{G}^{[86], [768]}$ :&{\tt lll\_word\_hidstate}\\
$\underline{I}^{[121], [768]}$ :&{\tt lll\_hidstate}\\
$\underline{L}^{[86], [6]}$ :&{\tt lll\_word\_score}\\
$\underline{M}^{[86], [300]}$ :&{\tt lll\_word\_hidstate}\\
$\underline{S}^{[86], [768]}$ :&{\tt lll\_word\_hidstate}\\
$\underline{X}^{[86], [6]}$ :&{\tt lll\_word\_score}
\end{tabular}



\begin{subequations}

\begin{equation}\color{blue}
\begin{aligned}
a^{[86]} &= \text{argmax}(X^{[86], [6]};dim=-1)
\label{eq-a-fun-sentence-ax-bnet}
\\ &:{\tt ll\_greedy\_ilabel}
\end{aligned}
\end{equation}

\begin{equation}\color{blue}
\begin{aligned}
B^{[121], [768]} &= \text{BERT}()
\label{eq-B-fun-sentence-ax-bnet}
\\ &:{\tt lll\_hidstate}
\end{aligned}
\end{equation}

\begin{equation}\color{blue}
\begin{aligned}
d^{[121], [768]} &= \text{dropout}(I^{[121], [768]})
\label{eq-d-fun-sentence-ax-bnet}
\\ &:{\tt lll\_hidstate}
\end{aligned}
\end{equation}

\begin{equation}\color{blue}
\begin{aligned}
E^{[86], [768]} &= \text{embedding}(a^{[86]})
\label{eq-E-fun-sentence-ax-bnet}
\\ &:{\tt lll\_pred\_code}
\end{aligned}
\end{equation}

\begin{equation}\color{blue}
\begin{aligned}
G^{[86], [768]} &= \text{gather}(d^{[121], [768]};dim=-2)
\label{eq-G-fun-sentence-ax-bnet}
\\ &:{\tt lll\_word\_hidstate}
\end{aligned}
\end{equation}

\begin{equation}\color{blue}
\begin{aligned}
I^{[121], [768]} &= \left[B^{[121], [768]}\indi(depth=0) M^{[86], [300]}\indi(depth> 0)\right]
\label{eq-I-fun-sentence-ax-bnet}
\\ &:{\tt lll\_hidstate}
\end{aligned}
\end{equation}

\begin{equation}\color{blue}
\begin{aligned}
L^{[86], [6]} &= M^{[86], [300]}W_{il}^{[300],[6]}
\label{eq-L-fun-sentence-ax-bnet}
\\ &:{\tt lll\_word\_score}
\end{aligned}
\end{equation}

\begin{equation}\color{blue}
\begin{aligned}
M^{[86], [300]} &= \left[G^{[86], [768]}\indi(depth=0) + S^{[86], [768]}\indi(depth> 0) \right] W_{me}^{[768], [300]}
\label{eq-M-fun-sentence-ax-bnet}
\\ &:{\tt lll\_word\_hidstate}
\end{aligned}
\end{equation}

\begin{equation}\color{blue}
\begin{aligned}
S^{[86], [768]} &= E^{[86], [768]} + G^{[86], [768]}
\label{eq-S-fun-sentence-ax-bnet}
\\ &:{\tt lll\_word\_hidstate}
\end{aligned}
\end{equation}

\begin{equation}\color{blue}
\begin{aligned}
X^{[86], [6]} &= L^{[86], [6]}\indi(depth> 0)
\label{eq-X-fun-sentence-ax-bnet}
\\ &:{\tt lll\_word\_score}
\end{aligned}
\end{equation}

\end{subequations}


\end{document}  
